{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ji09xLvKqBBd"
      },
      "source": [
        "<p> ################################################################## </p>\n",
        "<p>#&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;#\n",
        "<p> # This source code is adopted from Tomaz and modified for NWMSU chatbot application  &nbsp; #</p>\n",
        "<p>#&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;#\n",
        "<p></p>\n",
        "<p></p>\n",
        "<p> ################################################################## </p>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zqgaHIIWAv0E"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y8W6QNR3xzEO",
        "outputId": "268fe16d-2d47-4594-bf63-346d45f38ff6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n",
            "/content/gdrive/My Drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "%cd gdrive/My Drive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_5jQBSpUA1YJ"
      },
      "source": [
        "Requirements:\n",
        "\n",
        "1. **LangChain**: A framework for developing applications powered by language models.\n",
        "2. **LangChain-Community**: A collaborative space for sharing resources, tools, and discussions related to LangChain.\n",
        "3. **LangChain-OpenAI**: A LangChain extension providing integration with OpenAI's language models.\n",
        "4. **LangChain-Experimental**: A repository for experimental features and prototypes within the LangChain ecosystem.\n",
        "5. **Neo4j**: A graph database management system designed for handling and querying connected data.\n",
        "6. **tiktoken**: A library for tokenizing text, commonly used with language models to preprocess input and output.\n",
        "7. **yfiles_jupyter_graphs**: A library for creating and visualizing graphs and network diagrams in Jupyter notebooks.\n",
        "8. **Streamlit**: An open-source framework for building interactive, web-based applications using Python.\n",
        "9. **Localtunnel**: A tool that allows you to expose your local web server to the internet via a secure tunnel.\n",
        "10. **CTransformers** : A python wrapper for transfomer based models with essential configurations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_e8mFtjUpsf-",
        "outputId": "ffdae928-d0e9-4749-cd2a-e516432610e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.4/62.4 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.2/209.2 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m312.3/312.3 kB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m52.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.7/15.7 MB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.8/9.8 MB\u001b[0m \u001b[31m43.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m68.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m48.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "%pip install --upgrade --quiet  langchain langchain-community langchain-openai langchain-experimental neo4j tiktoken yfiles_jupyter_graphs streamlit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uFTCrVRKSfls",
        "outputId": "dc9d2c9c-f32e-4ffa-f2cb-ee95af0503a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ctransformers\n",
            "  Downloading ctransformers-0.2.27-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.11/dist-packages (from ctransformers) (0.30.2)\n",
            "Requirement already satisfied: py-cpuinfo<10.0.0,>=9.0.0 in /usr/local/lib/python3.11/dist-packages (from ctransformers) (9.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->ctransformers) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->ctransformers) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->ctransformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->ctransformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->ctransformers) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->ctransformers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->ctransformers) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub->ctransformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub->ctransformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub->ctransformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub->ctransformers) (2025.1.31)\n",
            "Downloading ctransformers-0.2.27-py3-none-any.whl (9.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ctransformers\n",
            "Successfully installed ctransformers-0.2.27\n"
          ]
        }
      ],
      "source": [
        "pip install ctransformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wQpoIlvoWU-Y",
        "outputId": "4e2ec92b-57f0-4a60-c52c-62c82c8e8b70"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: neo4j in /usr/local/lib/python3.11/dist-packages (5.28.1)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.11/dist-packages (from neo4j) (2025.2)\n"
          ]
        }
      ],
      "source": [
        "pip install neo4j"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EKRc3oenyXvO",
        "outputId": "93508724-83d4-4dd0-e8e5-c4574fa6d9bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K\n",
            "up to date, audited 23 packages in 4s\n",
            "\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K\n",
            "\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K3 packages are looking for funding\n",
            "\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K  run `npm fund` for details\n",
            "\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K\n",
            "2 \u001b[31m\u001b[1mhigh\u001b[22m\u001b[39m severity vulnerabilities\n",
            "\n",
            "To address all issues (including breaking changes), run:\n",
            "  npm audit fix --force\n",
            "\n",
            "Run `npm audit` for details.\n",
            "\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K"
          ]
        }
      ],
      "source": [
        "!npm install localtunnel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LQtND_fzFRNI"
      },
      "source": [
        "The provided code imports several components from different libraries, each serving a specific purpose. The `RunnableBranch`, `RunnableLambda`, `RunnableParallel`, and `RunnablePassthrough` from LangChain Core are utilities for creating complex workflows involving branching logic, function definitions, parallel execution, and straightforward data passing. For crafting prompts, `ChatPromptTemplate` and `PromptTemplate` are used to design structured inputs for language models.\n",
        "\n",
        "The `BaseModel` and `Field` from Pydantic assist in data validation and configuration, while typing utilities like `Tuple`, `List`, and `Optional` are used for type hinting in Python code. The message classes `AIMessage` and `HumanMessage` define interactions between an AI and a user. The `StrOutputParser` is employed for converting outputs into string format.\n",
        "\n",
        "The `os` module provides a way to interact with the operating system, and `Neo4jGraph` facilitates working with Neo4j databases. `TokenTextSplitter` helps in breaking down text into manageable tokens, essential for language processing. `ChatOpenAI` enables communication with OpenAI's chat models, and `LLMGraphTransformer` is used for transforming graph data with language models. For database interaction, `GraphDatabase` connects with Neo4j, and `GraphWidget` allows for the creation and visualization of graphs in Jupyter notebooks.\n",
        "\n",
        "Further, `Neo4jVector` supports storing and retrieving embeddings in Neo4j, while `OpenAIEmbeddings` provides tools for generating embeddings from OpenAI models. The `remove_lucene_chars` function cleans text by removing specific characters, ensuring data is properly formatted for Neo4j. Lastly, `ConfigurableField` allows for configurable data fields within a runnable, aiding in the customization of workflows."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "n5jFoh3cqFfH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3bb3c8b8-b7bb-47a0-8d42-b5829826e770"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py:3553: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
            "\n",
            "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
            "with: `from pydantic import BaseModel`\n",
            "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
            "\n",
            "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
          ]
        }
      ],
      "source": [
        "# %%writefile main.py\n",
        "\n",
        "from langchain_core.runnables import (\n",
        "    RunnableBranch,\n",
        "    RunnableLambda,\n",
        "    RunnableParallel,\n",
        "    RunnablePassthrough,\n",
        ")\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.prompts.prompt import PromptTemplate\n",
        "from langchain_core.pydantic_v1 import BaseModel, Field\n",
        "from typing import Tuple, List, Optional\n",
        "from langchain_core.messages import AIMessage, HumanMessage\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "import os\n",
        "from langchain_community.graphs import Neo4jGraph\n",
        "from langchain.text_splitter import TokenTextSplitter\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_experimental.graph_transformers import LLMGraphTransformer\n",
        "from neo4j import GraphDatabase\n",
        "from yfiles_jupyter_graphs import GraphWidget\n",
        "from langchain_community.vectorstores import Neo4jVector\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_community.vectorstores.neo4j_vector import remove_lucene_chars\n",
        "from langchain_core.runnables import ConfigurableField, RunnableParallel, RunnablePassthrough\n",
        "\n",
        "\n",
        "try:\n",
        "  import google.colab\n",
        "  from google.colab import output\n",
        "  output.enable_custom_widget_manager()\n",
        "except:\n",
        "  pass\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-n8hzmiPGDkr"
      },
      "source": [
        "These lines of code set environment variables for accessing the OpenAI API and a Neo4j database:\n",
        "\n",
        "- `os.environ[\"OPENAI_API_KEY\"] = \"sk-....\"` sets the API key for accessing OpenAI's services.\n",
        "- `os.environ[\"NEO4J_URI\"] = \"bolt+s://d......databases.neo4j.io:7687\"` specifies the URI for connecting to the Neo4j database.\n",
        "- `os.environ[\"NEO4J_USERNAME\"] = \"usr\"` sets the username for the Neo4j database connection.\n",
        "- `os.environ[\"NEO4J_PASSWORD\"] = \"pwd\"` sets the password for the Neo4j database connection.\n",
        "\n",
        "These environment variables are crucial for securely storing sensitive information required for connecting to external services."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "u-x0sDvkzApK"
      },
      "outputs": [],
      "source": [
        "os.environ[\"OPENAI_API_KEY\"] = \"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\"#replace your OPEN_API key\n",
        "os.environ[\"NEO4J_URI\"] = \"neo4j+s://dac05e46.databases.neo4j.io\"#NEO4J URL\n",
        "os.environ[\"NEO4J_USERNAME\"] = \"neo4j\" #NEO4J USERNAME\n",
        "os.environ[\"NEO4J_PASSWORD\"] = \"bGtmyZOubMOdqv8NIpT74GJGVcF4VUbngRnzCnHvRMU\"#NEO4J PASSWORD\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W3SvhzJ4GIM2"
      },
      "source": [
        "This instance, stored in the variable graph, can be used to interact with the Neo4j database. The Neo4jGraph class provides methods and functionalities to execute queries, retrieve data, and manipulate the graph database, facilitating seamless integration and operations within the LangChain framework."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "XwzkR_GOzArj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9b9afcd-8a42-4af6-c2da-a5fce561dcdc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-6e7385464d00>:1: LangChainDeprecationWarning: The class `Neo4jGraph` was deprecated in LangChain 0.3.8 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-neo4j package and should be used instead. To use it run `pip install -U :class:`~langchain-neo4j` and import as `from :class:`~langchain_neo4j import Neo4jGraph``.\n",
            "  graph = Neo4jGraph()\n"
          ]
        }
      ],
      "source": [
        "graph = Neo4jGraph()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iw1cepLUGWEM"
      },
      "source": [
        "This code snippet loads web pages from a list of URLs using the `WebBaseLoader` class from the `langchain_community.document_loaders` module:\n",
        "\n",
        "\n",
        "- **WebBaseLoader**: This class is used for loading web pages from URLs provided in a list. It likely uses web scraping techniques to retrieve the content from each URL.\n",
        "- **loader.requests_kwargs**: This line sets a keyword argument for the HTTP request made by the loader. `{'verify': False}` disables SSL certificate verification, which can be useful when dealing with self-signed certificates or testing environments.\n",
        "- **data = loader.load()**: This line executes the loading process, fetching the HTML content from the specified URLs and storing it in the `data` variable.\n",
        "\n",
        "This approach is helpful for gathering content from multiple web pages for further processing, such as natural language processing or content analysis, within the LangChain framework."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nfpW5QoAudoX",
        "outputId": "b0ea81a8-d0ad-4eb9-91ca-4f3e21095c0e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'www.imdb.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'www.imdb.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'www.imdb.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'www.imdb.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'www.imdb.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "loader = WebBaseLoader([\"https://www.imdb.com/\",\n",
        "                        \"https://www.imdb.com/chart/top/?ref_=nv_mv_250\",\n",
        "                        \"https://www.imdb.com/chart/moviemeter/?ref_=nv_mv_mpm\",\n",
        "                        \"https://www.imdb.com/interest/all/?ref_=nv_ch_gr\",\n",
        "                        \"https://www.imdb.com/chart/boxoffice/?ref_=nv_ch_cht\"\n",
        "                        ])\n",
        "loader.requests_kwargs = {'verify':False}\n",
        "data = loader.load()\n",
        "# print(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T4Fcg-eBv2AB",
        "outputId": "6efdd294-7864-4047-b9c1-06a6dbfee41f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4255"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "len(data[0].page_content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "fOWYO20aLVBL",
        "outputId": "9d17614c-f3a9-43d5-fe6d-9c26d96c04ae"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'IMDb: Ratings, Reviews, and Where to Watch the Best Movies & TV Shows \\n\\n\\nMenuMoviesRelease CalendarT'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "data[0].page_content[0:100]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LNSNunP2Gd2c"
      },
      "source": [
        "\n",
        "\n",
        "- **TokenTextSplitter**: This class is used to split text into chunks, typically to prepare it for processing by language models or other natural language processing tasks.\n",
        "- **text_splitter = TokenTextSplitter(chunk_size=512, chunk_overlap=24)**: Initializes an instance of `TokenTextSplitter` with a chunk size of 512 tokens and an overlap of 24 tokens between consecutive chunks.\n",
        "- **documents = text_splitter.split_documents(data)**: Splits the input `data` (presumably a collection of text documents) into chunks of the specified size and overlap, storing the resulting chunks in the `documents` variable.\n",
        "\n",
        "This approach is useful for breaking down large text documents or datasets into manageable pieces that can be processed more efficiently by downstream tasks or models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "WSBwTblIxmgO"
      },
      "outputs": [],
      "source": [
        "text_splitter = TokenTextSplitter(chunk_size=512, chunk_overlap=24)\n",
        "documents = text_splitter.split_documents(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dI5DbzHLezVX",
        "outputId": "0b702fc3-8d01-4a8d-e6cc-349975f44d6c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "page_content='IMDb: Ratings, Reviews, and Where to Watch the Best Movies & TV Shows \n",
            "\n",
            "\n",
            "MenuMoviesRelease CalendarTop 250 MoviesMost Popular MoviesBrowse Movies by GenreTop Box OfficeShowtimes & TicketsMovie NewsIndia Movie SpotlightTV ShowsWhat's on TV & StreamingTop 250 TV ShowsMost Popular TV ShowsBrowse TV Shows by GenreTV NewsWatchWhat to WatchLatest TrailersIMDb OriginalsIMDb PicksIMDb SpotlightIMDb PodcastsAwards & EventsOscarsCannes Film FestivalStar WarsSXSW Film FestivalSTARmeter AwardsAwards CentralFestival CentralAll EventsCelebsBorn TodayMost Popular CelebsCelebrity NewsCommunityHelp CenterContributor ZonePollsFor Industry ProfessionalsLanguageEnglish (United States)LanguageFully supportedEnglish (United States)Partially supportedFrançais (Canada)Français (France)Deutsch (Deutschland)हिंदी (भारत)Italiano (Italia)Português (Brasil)Español (España)Español (México)AllAllWatchlistSign InENFully supportedEnglish (United States)Partially supportedFrançais (Canada)Français (France)Deutsch (Deutschland)हिंदी (भारत)Italiano (Italia)Português (Brasil)Español (España)Español (México)Use app\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "2:06\"Wednesday\" Returns to Netflix on August 62:06Watch the Trailer9:10How Well Does 'The Accountant 2' Cast Know Each Other?9:10Ben Affleck and Jon Bernthal Face Off2:19'Fountain of Youth'2:19Watch the Trailer3:23How Ryan Coogler Conjured Magic While Making 'Sinners'3:23Watch the Interview5:01Diego Luna on Cassian's Growth in \"Andor\" Season 25:01Watch Our Star Wars Celebration Interview9:50Minecraft ... or Mythcraft?9:50We Put Jack Black & Jason Momoa to the Test2:03'28 Years Later'2:03Watch the New Trailer4:07Bowen Yang Talks 'The Wedding Banquet,' 'Wicked,' and More4:' metadata={'source': 'https://www.imdb.com/', 'title': 'IMDb: Ratings, Reviews, and Where to Watch the Best Movies & TV Shows', 'description': \"IMDb is the world's most popular and authoritative source for movie, TV and celebrity content. Find ratings and reviews for the newest movie and TV shows. Get personalized recommendations, and learn where to watch across hundreds of streaming providers.\", 'language': 'en-US'}\n",
            "page_content=' the New Trailer4:07Bowen Yang Talks 'The Wedding Banquet,' 'Wicked,' and More4:07Watch Our Sundance Studio Interview4:33The Rise of Pedro Pascal4:33From \"Buffy the Vampire Slayer\" to \"The Last of Us\"2:36Idris Elba and John Cena Are 'Heads of State'2:36Watch the Trailer0:54\"The Summer I Turned Pretty\"0:54Watch the Season 3 Teaser2:45'The Fantastic Four: First Steps'2:45Watch the New Trailer1:39Nic Cage Is 'The Surfer'1:39Watch an Exclusive Clip1:12'Predator: Badlands'1:12Watch the Trailer4:54Penn Badgley Shares Favorite \"You\" Episode4:54Watch the InterviewUp next4:54Penn Badgley Shares Favorite \"You\" EpisodeWatch the Interview2:06\"Wednesday\" Returns to Netflix on August 6Watch the Trailer9:10How Well Does 'The Accountant 2' Cast Know Each Other?Ben Affleck and Jon Bernthal Face Off2:19'Fountain of Youth'Watch the Trailer3:23How Ryan Coogler Conjured Magic While Making 'Sinners'Watch the Interview5:01Diego Luna on Cassian's Growth in \"Andor\" Season 2Watch Our Star Wars Celebration InterviewBrowse trailersFeatured todayListStaff Picks: What to Watch in AprilSee our picksPhotosHollywood Power CouplesSee the galleryListEvery Star Wars Movie and Series, RankedSee the listListTV Tracker: Renewed and Canceled ShowsCheck the statusPhotosTom Hardy Through the YearsSee the galleryPhotosThe Latest Movie and TV Series PostersSee more postersPhotosPhotos We Love: Lisa at Coachella and BeyondSee the galleryPhotosPortrayals of the Pope On ScreenSee the full gallery\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Most popular celebritiesWhat to watchGet more recommendationsExclusive videosIMDb OriginalsCelebrity interviews, trending entertainment stories, and expert analysis4:33The Rise of Pedro PascalWatch the video4:54Penn Badgley Shares His Favorite \"You\" EpisodeWatch now6:18\"The Last of Us\" Cast's Favorite MomentsWatch the interview3:08Why the  'Mickey 17' Cast Loves Bong Joon HoWatch now4:36The 'Warfare' Cast on the Beauty of Boot CampWatch the interview9:50Jack Black and Jason Momoa Play' metadata={'source': 'https://www.imdb.com/', 'title': 'IMDb: Ratings, Reviews, and Where to Watch the Best Movies & TV Shows', 'description': \"IMDb is the world's most popular and authoritative source for movie, TV and celebrity content. Find ratings and reviews for the newest movie and TV shows. Get personalized recommendations, and learn where to watch across hundreds of streaming providers.\", 'language': 'en-US'}\n",
            "page_content=' 'Warfare' Cast on the Beauty of Boot CampWatch the interview9:50Jack Black and Jason Momoa Play \"Minecraft or Mythcraft?\"Watch the interview9:10How Well Do Ben and Jon Know Each Other?Watch now5:01Diego Luna on Cassian's Growth in \"Andor\" Season 2Watch the interview3:23How Ryan Coogler Conjured Magic With Music in 'Sinners'Watch the interviewExplore what’s streamingExplore Movies & TV showsMore to exploreRecently viewedYou have no recently viewed pagesGet the IMDb appSign in for more accessSign in for more accessFollow IMDb on socialGet the IMDb appFor Android and iOSHelpSite IndexIMDbProBox Office MojoLicense IMDb DataPress RoomAdvertisingJobsConditions of UsePrivacy PolicyYour Ads Privacy ChoicesIMDb, an Amazon company© 1990-2025 by IMDb.com, Inc.Back to top\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "' metadata={'source': 'https://www.imdb.com/', 'title': 'IMDb: Ratings, Reviews, and Where to Watch the Best Movies & TV Shows', 'description': \"IMDb is the world's most popular and authoritative source for movie, TV and celebrity content. Find ratings and reviews for the newest movie and TV shows. Get personalized recommendations, and learn where to watch across hundreds of streaming providers.\", 'language': 'en-US'}\n"
          ]
        }
      ],
      "source": [
        "print(documents[0])\n",
        "print(documents[1])\n",
        "print(documents[2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2MEovPDGzBW"
      },
      "source": [
        "\n",
        "\n",
        "- **ChatOpenAI**: This class is used to interact with OpenAI's chat models. In this case, it's initialized with parameters like `temperature=0` and `model_name=\"gpt-3.5-turbo-0125\"`.\n",
        "- **LLMGraphTransformer**: This class is used to transform text documents into graph documents using a language model (LLM).\n",
        "- **Neo4jGraph**: This class represents a connection to a Neo4j graph database.\n",
        "\n",
        "\n",
        "The code first initializes the language model (`llm`) and the transformer (`llm_transformer`) to convert text documents (`documents`) into graph documents. Then, it initializes a `Neo4jGraph` instance (`graph`) and adds the transformed graph documents to the Neo4j database using the `add_graph_documents` method.\n",
        "\n",
        "This approach integrates natural language processing with graph database operations, enabling the creation of structured graph representations from unstructured text data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "id": "0DttkbLsyAza",
        "outputId": "127f2d5e-b2f8-4584-8330-f36d44dbb7b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/langchain_openai/chat_models/base.py:1673: UserWarning: Cannot use method='json_schema' with model gpt-3.5-turbo-0125 since it doesn't support OpenAI's Structured Output API. You can see supported models here: https://platform.openai.com/docs/guides/structured-outputs#supported-models. To fix this warning, set `method='function_calling'. Overriding to method='function_calling'.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AuthenticationError",
          "evalue": "Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-proj-********************************************************************************************************************************************************TmIA. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAuthenticationError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-4909921aa873>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Following needs to be used only first time to load the data. It took 3m 44s\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mgraph_documents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mllm_transformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_graph_documents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_experimental/graph_transformers/llm.py\u001b[0m in \u001b[0;36mconvert_to_graph_documents\u001b[0;34m(self, documents, config)\u001b[0m\n\u001b[1;32m    930\u001b[0m             \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mGraphDocument\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mtransformed\u001b[0m \u001b[0mdocuments\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mgraphs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \"\"\"\n\u001b[0;32m--> 932\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdocument\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocuments\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    933\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    934\u001b[0m     async def aprocess_response(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_experimental/graph_transformers/llm.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    930\u001b[0m             \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mGraphDocument\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mtransformed\u001b[0m \u001b[0mdocuments\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mgraphs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \"\"\"\n\u001b[0;32m--> 932\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdocument\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocuments\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    933\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    934\u001b[0m     async def aprocess_response(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_experimental/graph_transformers/llm.py\u001b[0m in \u001b[0;36mprocess_response\u001b[0;34m(self, document, config)\u001b[0m\n\u001b[1;32m    837\u001b[0m         \"\"\"\n\u001b[1;32m    838\u001b[0m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdocument\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpage_content\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 839\u001b[0;31m         \u001b[0mraw_schema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"input\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    840\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_call\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    841\u001b[0m             \u001b[0mraw_schema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_schema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   3032\u001b[0m                         \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3033\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3034\u001b[0;31m                         \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3035\u001b[0m         \u001b[0;31m# finish the root run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3036\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   3758\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3759\u001b[0m                 ]\n\u001b[0;32m-> 3760\u001b[0;31m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfuture\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfutures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3761\u001b[0m         \u001b[0;31m# finish the root run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3762\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   3758\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3759\u001b[0m                 ]\n\u001b[0;32m-> 3760\u001b[0;31m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfuture\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfutures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3761\u001b[0m         \u001b[0;31m# finish the root run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3762\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    454\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 456\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    457\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    399\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 401\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    402\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m                 \u001b[0;31m# Break a reference cycle with the exception in self._exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/concurrent/futures/thread.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36m_invoke_step\u001b[0;34m(step, input, config, key)\u001b[0m\n\u001b[1;32m   3742\u001b[0m             )\n\u001b[1;32m   3743\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mset_config_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchild_config\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3744\u001b[0;31m                 return context.run(\n\u001b[0m\u001b[1;32m   3745\u001b[0m                     \u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3746\u001b[0m                     \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   5414\u001b[0m         \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5415\u001b[0m     ) -> Output:\n\u001b[0;32m-> 5416\u001b[0;31m         return self.bound.invoke(\n\u001b[0m\u001b[1;32m   5417\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5418\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_merge_configs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    366\u001b[0m         return cast(\n\u001b[1;32m    367\u001b[0m             \u001b[0;34m\"ChatGeneration\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 368\u001b[0;31m             self.generate_prompt(\n\u001b[0m\u001b[1;32m    369\u001b[0m                 \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m                 \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    935\u001b[0m     ) -> LLMResult:\n\u001b[1;32m    936\u001b[0m         \u001b[0mprompt_messages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_messages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprompts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 937\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt_messages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    938\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    939\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0moverride\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    757\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m                 results.append(\n\u001b[0;32m--> 759\u001b[0;31m                     self._generate_with_cache(\n\u001b[0m\u001b[1;32m    760\u001b[0m                         \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m                         \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36m_generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1000\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_from_stream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1001\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_generate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"run_manager\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1002\u001b[0;31m             result = self._generate(\n\u001b[0m\u001b[1;32m   1003\u001b[0m                 \u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1004\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_openai/chat_models/base.py\u001b[0m in \u001b[0;36m_generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    989\u001b[0m             \u001b[0mgeneration_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"headers\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    990\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 991\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mpayload\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    992\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_chat_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgeneration_info\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    993\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_utils/_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m                         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Missing required argument: {quote(missing[0])}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/resources/chat/completions/completions.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    927\u001b[0m     ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[1;32m    928\u001b[0m         \u001b[0mvalidate_response_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 929\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m    930\u001b[0m             \u001b[0;34m\"/chat/completions\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m             body=maybe_transform(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1274\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1275\u001b[0m         )\n\u001b[0;32m-> 1276\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1278\u001b[0m     def patch(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    947\u001b[0m             \u001b[0mretries_taken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    948\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 949\u001b[0;31m         return self._request(\n\u001b[0m\u001b[1;32m    950\u001b[0m             \u001b[0mcast_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1055\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1056\u001b[0m             \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Re-raising status error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1057\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_status_error_from_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1058\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1059\u001b[0m         return self._process_response(\n",
            "\u001b[0;31mAuthenticationError\u001b[0m: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-proj-********************************************************************************************************************************************************TmIA. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}"
          ]
        }
      ],
      "source": [
        "llm=ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo-0125\") # gpt-4-0125-preview occasionally has issues\n",
        "llm_transformer = LLMGraphTransformer(llm=llm)\n",
        "\n",
        "# Following needs to be used only first time to load the data. It took 3m 44s\n",
        "graph_documents = llm_transformer.convert_to_graph_documents(documents)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nW1WQB5-PYhj"
      },
      "source": [
        "**The following needs to be run for the first time when you load data into DB.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "qxodjNISPSXK"
      },
      "outputs": [],
      "source": [
        "graph.add_graph_documents(\n",
        "    graph_documents,\n",
        "    baseEntityLabel=True,\n",
        "    include_source=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        },
        "id": "zwUdvx8I5208",
        "outputId": "f30515b5-d721-418e-b04f-6d303f1b8c33"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "list index out of range",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-c6c39cadeb5c>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph_documents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ],
      "source": [
        "print (documents[5])\n",
        "print(graph_documents[5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-76PrdloHXmK"
      },
      "source": [
        "The `showGraph` function takes an optional `Cypher `query string as input (defaulting to a predefined query), establishes a connection to `Neo4j` using credentials from environment variables, executes the query to retrieve graph data, and then visualizes it using a `GraphWidget`. The widget allows interactive exploration of the graph nodes and relationships directly within the notebook environment, making it easy to analyze and understand graph structures and connections."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 817,
          "referenced_widgets": [
            "4fac87ddb3c24ea899f27c9dc57f42bf",
            "83960149f86840c4b4a1a55052e1cecd"
          ]
        },
        "id": "verAasH9yIu6",
        "outputId": "6b5cf750-cf8d-40d1-bb6d-ac8ac13aa511"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4fac87ddb3c24ea899f27c9dc57f42bf",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "GraphWidget(layout=Layout(height='800px', width='100%'))"
            ]
          },
          "metadata": {
            "application/vnd.jupyter.widget-view+json": {
              "colab": {
                "custom_widget_manager": {
                  "url": "https://ssl.gstatic.com/colaboratory-static/widgets/colab-cdn-widget-manager/2b70e893a8ba7c0f/manager.min.js"
                }
              }
            }
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "# directly show the graph resulting from the given Cypher query\n",
        "default_cypher = \"MATCH (s)-[r:!MENTIONS]->(t) RETURN s,r,t LIMIT 50\"\n",
        "\n",
        "def showGraph(cypher: str = default_cypher):\n",
        "    # create a neo4j session to run queries\n",
        "    driver = GraphDatabase.driver(\n",
        "        uri = os.environ[\"NEO4J_URI\"],\n",
        "        auth = (os.environ[\"NEO4J_USERNAME\"],\n",
        "                os.environ[\"NEO4J_PASSWORD\"]))\n",
        "    session = driver.session()\n",
        "    widget = GraphWidget(graph = session.run(cypher).graph())\n",
        "    widget.node_label_mapping = 'id'\n",
        "    #display(widget)\n",
        "    return widget\n",
        "\n",
        "showGraph()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2LSQjHkAHuTj"
      },
      "source": [
        "The `Neo4jVector.from_existing_graph()` method is used to create a vector index within a Neo4j database. It integrates embeddings from OpenAI's models, facilitating efficient search and retrieval operations based on both textual content and embedding similarities.\n",
        "\n",
        "### Key Components:\n",
        "\n",
        "- **LangChain Components**:\n",
        "  - **Neo4jVector**: This class from LangChain Community is designed to manage vector indexes within Neo4j, enabling storage and retrieval of embeddings associated with graph nodes.\n",
        "  - **OpenAIEmbeddings**: This class from LangChain OpenAI provides tools for generating embeddings using OpenAI's models.\n",
        "\n",
        "- **Initialization Parameters**:\n",
        "  - **OpenAIEmbeddings()**: Initializes the use of OpenAI's models to generate embeddings.\n",
        "  - **search_type=\"hybrid\"**: Specifies a hybrid search approach, leveraging both textual content and embedding similarities for efficient querying.\n",
        "  - **node_label=\"Document\"**: Specifies that nodes labeled \"Document\" in the Neo4j graph will store document-related information.\n",
        "  - **text_node_properties=[\"text\"]**: Defines the property name(s) within \"Document\" nodes where textual content is stored.\n",
        "  - **embedding_node_property=\"embedding\"**: Specifies the property name where embeddings are stored within the \"Document\" nodes.\n",
        "\n",
        "### Usage Scenario:\n",
        "\n",
        "This setup is useful in scenarios where you want to store and query documents based on both their textual content and their embeddings. For example, you can store documents in Neo4j, extract their embeddings using OpenAI's models, and then use this vector index to efficiently search for documents based on their semantic similarity to a given query or to other documents in the database.\n",
        "\n",
        "This approach leverages the capabilities of Neo4j for graph-based storage and retrieval, coupled with advanced embedding techniques from OpenAI, providing a powerful toolset for managing and analyzing textual data within a graph database environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J23HXjaEzdu8"
      },
      "outputs": [],
      "source": [
        "vector_index = Neo4jVector.from_existing_graph(\n",
        "    OpenAIEmbeddings(),\n",
        "    search_type=\"hybrid\",\n",
        "    node_label=\"Document\",\n",
        "    text_node_properties=[\"text\"],\n",
        "    embedding_node_property=\"embedding\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U__qaE9sHtW8"
      },
      "source": [
        "\n",
        "\n",
        "### Retriever Setup:\n",
        "\n",
        "The code snippet performs the following actions:\n",
        "\n",
        "1. **Create Fulltext Index**:\n",
        "   - The `graph.query()` statement creates a fulltext index named `entity` if it does not already exist. This index is applied to nodes labeled `__Entity__` and indexes the `id` property of these nodes for efficient querying.\n",
        "\n",
        "2. **Entity Data Model**:\n",
        "   - The `Entities` class, derived from `BaseModel`, defines a structured representation for identifying information about entities extracted from text.\n",
        "   - **Names**: A list of strings representing entities such as courses, professors, requirements, or project entities found in the text.\n",
        "\n",
        "3. **Chat Prompt Template**:\n",
        "   - The `ChatPromptTemplate.from_messages()` method initializes a prompt template for interacting with users.\n",
        "   - The prompt consists of a system message informing the user about the extraction task and a human message specifying the format for input to extract information.\n",
        "\n",
        "4. **Entity Chain**:\n",
        "   - The `prompt | llm.with_structured_output(Entities)` constructs an entity extraction pipeline using LangChain:\n",
        "     - **Prompt**: Provides instructions to the user on how to input questions or text for entity extraction.\n",
        "     - **LLM (Language Model)**: Processes the input using OpenAI's language model to extract structured entities as defined by the `Entities` class.\n",
        "\n",
        "### Usage:\n",
        "\n",
        "This setup is designed to facilitate the extraction of entities from text input in a structured and systematic manner. The fulltext index ensures that entity nodes in the Neo4j graph are efficiently indexed, allowing for quick retrieval of relevant entities based on their `id` property. The `Entities` data model and the prompt template guide the interaction with users, ensuring that the extraction process is clear and consistent.\n",
        "\n",
        "This approach leverages natural language processing and graph database capabilities to manage and analyze textual data effectively, supporting tasks such as information extraction, entity linking, and knowledge base population within a broader application or system."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4DLAEMgHz2nF",
        "outputId": "e88cb0f7-9a60-4e62-9285-24f92c2a00f3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/langchain_openai/chat_models/base.py:1569: UserWarning: Received a Pydantic BaseModel V1 schema. This is not supported by method=\"json_schema\". Please use method=\"function_calling\" or specify schema via JSON Schema or Pydantic V2 BaseModel. Overriding to method=\"function_calling\".\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/langchain_openai/chat_models/base.py:1582: UserWarning: Cannot use method='json_schema' with model gpt-3.5-turbo-0125 since it doesn't support OpenAI's Structured Output API. You can see supported models here: https://platform.openai.com/docs/guides/structured-outputs#supported-models. To fix this warning, set `method='function_calling'. Overriding to method='function_calling'.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Retriever\n",
        "\n",
        "graph.query(\n",
        "    \"CREATE FULLTEXT INDEX entity IF NOT EXISTS FOR (e:__Entity__) ON EACH [e.id]\")\n",
        "\n",
        "# Extract entities from text\n",
        "class Entities(BaseModel):\n",
        "    \"\"\"Identifying information about entities.\"\"\"\n",
        "\n",
        "    names: List[str] = Field(\n",
        "        ...,\n",
        "        description=\"All the courses, requirements, or business entities that \"\n",
        "        \"appear in the text\",\n",
        "    )\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"You are extracting course, professors, requirements, project entities from the text.\",\n",
        "        ),\n",
        "        (\n",
        "            \"human\",\n",
        "            \"Use the given format to extract information from the following \"\n",
        "            \"input: {question}\",\n",
        "        ),\n",
        "    ]\n",
        ")\n",
        "\n",
        "entity_chain = prompt | llm.with_structured_output(Entities)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_JKJNR3YWU-p"
      },
      "source": [
        "### Description\n",
        "\n",
        "This script is used to retrieve related information from a Neo4j graph database based on user input.\n",
        "\n",
        "#### Functions:\n",
        "\n",
        "- **`generate_full_text_query(input: str)`**  \n",
        "  Converts a given input string into a fuzzy full-text search query.  \n",
        "  - Splits the input into individual words.\n",
        "  - Appends a similarity threshold (`~2`) to each word to allow minor misspellings.\n",
        "  - Combines the words using the `AND` operator for more accurate matching.\n",
        "\n",
        "- **`structured_retriever(question: str)`**  \n",
        "  Extracts entities from a natural language question and fetches related nodes from the graph.  \n",
        "  - Uses `entity_chain` to extract key entity names from the question.\n",
        "  - For each entity, performs a full-text search using the generated query.\n",
        "  - Retrieves up to 50 paths showing relationships to and from each matching node.\n",
        "  - Returns the results as a formatted string."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V1jRiI6G0J6P"
      },
      "outputs": [],
      "source": [
        "def generate_full_text_query(input: str) -> str:\n",
        "    \"\"\"\n",
        "    Generate a full-text search query for a given input string.\n",
        "\n",
        "    This function constructs a query string suitable for a full-text search.\n",
        "    It processes the input string by splitting it into words and appending a\n",
        "    similarity threshold (~2 changed characters) to each word, then combines\n",
        "    them using the AND operator. Useful for mapping entities from user questions\n",
        "    to database values, and allows for some misspelings.\n",
        "    \"\"\"\n",
        "    full_text_query = \"\"\n",
        "    words = [el for el in remove_lucene_chars(input).split() if el]\n",
        "    for word in words[:-1]:\n",
        "        full_text_query += f\" {word}~2 AND\"\n",
        "    full_text_query += f\" {words[-1]}~2\"\n",
        "    return full_text_query.strip()\n",
        "\n",
        "# Fulltext index query\n",
        "def structured_retriever(question: str) -> str:\n",
        "    \"\"\"\n",
        "    Collects the neighborhood of entities mentioned\n",
        "    in the question\n",
        "    \"\"\"\n",
        "    result = \"\"\n",
        "    entities = entity_chain.invoke({\"question\": question})\n",
        "    for entity in entities.names:\n",
        "        response = graph.query(\n",
        "            \"\"\"CALL db.index.fulltext.queryNodes('entity', $query, {limit:2})\n",
        "            YIELD node,score\n",
        "            CALL {\n",
        "              WITH node\n",
        "              MATCH (node)-[r:!MENTIONS]->(neighbor)\n",
        "              RETURN node.id + ' - ' + type(r) + ' -> ' + neighbor.id AS output\n",
        "              UNION ALL\n",
        "              WITH node\n",
        "              MATCH (node)<-[r:!MENTIONS]-(neighbor)\n",
        "              RETURN neighbor.id + ' - ' + type(r) + ' -> ' +  node.id AS output\n",
        "            }\n",
        "            RETURN output LIMIT 50\n",
        "            \"\"\",\n",
        "            {\"query\": generate_full_text_query(entity)},\n",
        "        )\n",
        "        result += \"\\n\".join([el['output'] for el in response])\n",
        "    return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aDRKabxJWU-q"
      },
      "source": [
        "- **`retriever(question: str)`**  \n",
        "  Combines results from both structured and unstructured data sources to answer a user question.  \n",
        "  - Logs the original search query.\n",
        "  - Uses `structured_retriever` to fetch entity-related information from a Neo4j graph (structured data).\n",
        "  - Performs a similarity search over a vector index to retrieve relevant documents (unstructured data).\n",
        "  - Formats and returns both data types in a combined response.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C1ftDuLy0P8X"
      },
      "outputs": [],
      "source": [
        "def retriever(question: str):\n",
        "    print(f\"Search query: {question}\")\n",
        "    structured_data = structured_retriever(question)\n",
        "    unstructured_data = [el.page_content for el in vector_index.similarity_search(question)]\n",
        "    final_data = f\"\"\"Structured data:\n",
        "{structured_data}\n",
        "Unstructured data:\n",
        "{\"#Document \". join(unstructured_data)}\n",
        "    \"\"\"\n",
        "    return final_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_-vBRUQWU-r"
      },
      "source": [
        "### Condensing Chat History for Standalone Question Generation\n",
        "\n",
        "This code helps rephrase a follow-up question into a standalone question by considering the previous chat history.\n",
        "\n",
        "#### Components:\n",
        "\n",
        "- **`_template` and `CONDENSE_QUESTION_PROMPT`**  \n",
        "  Defines a prompt template used to generate a standalone question from a follow-up query and previous chat context.\n",
        "\n",
        "- **`_format_chat_history(chat_history: List[Tuple[str, str]]) -> List`**  \n",
        "  Converts a list of chat history (pairs of human and AI messages) into a format compatible with LLMs, using `HumanMessage` and `AIMessage` objects.\n",
        "\n",
        "- **`_search_query`**  \n",
        "  A conditional chain (`RunnableBranch`) that:\n",
        "  - If chat history exists:\n",
        "    - Formats the history.\n",
        "    - Fills the prompt with the history and the follow-up question.\n",
        "    - Sends it to an LLM (e.g., OpenAI) to generate a standalone question.\n",
        "  - If no chat history exists:\n",
        "    - Returns the original question as-is.\n",
        "\n",
        "This setup is typically used to improve retrieval or question answering pipelines by ensuring queries are self-contained.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "azltkyJN0U_I"
      },
      "outputs": [],
      "source": [
        "# Condense a chat history and follow-up question into a standalone question\n",
        "_template = \"\"\"Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question,\n",
        "in its original language.\n",
        "Chat History:\n",
        "{chat_history}\n",
        "Follow Up Input: {question}\n",
        "Standalone question:\"\"\"  # noqa: E501\n",
        "CONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(_template)\n",
        "\n",
        "def _format_chat_history(chat_history: List[Tuple[str, str]]) -> List:\n",
        "    buffer = []\n",
        "    for human, ai in chat_history:\n",
        "        buffer.append(HumanMessage(content=human))\n",
        "        buffer.append(AIMessage(content=ai))\n",
        "    return buffer\n",
        "\n",
        "_search_query = RunnableBranch(\n",
        "    # If input includes chat_history, we condense it with the follow-up question\n",
        "    (\n",
        "        RunnableLambda(lambda x: bool(x.get(\"chat_history\"))).with_config(\n",
        "            run_name=\"HasChatHistoryCheck\"\n",
        "        ),  # Condense follow-up question and chat into a standalone_question\n",
        "        RunnablePassthrough.assign(\n",
        "            chat_history=lambda x: _format_chat_history(x[\"chat_history\"])\n",
        "        )\n",
        "        | CONDENSE_QUESTION_PROMPT\n",
        "        | ChatOpenAI(temperature=0)\n",
        "        | StrOutputParser(),\n",
        "    ),\n",
        "    # Else, we have no chat history, so just pass through the question\n",
        "    RunnableLambda(lambda x : x[\"question\"]),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tpA9dhbRWU-6"
      },
      "source": [
        "### Answering a Question Using Context-Aware Retrieval\n",
        "\n",
        "This code defines a chain that answers a user question based only on retrieved context from structured and unstructured sources.\n",
        "\n",
        "#### Components:\n",
        "\n",
        "- **`template` and `prompt`**  \n",
        "  A prompt template that:\n",
        "  - Inserts the relevant context and the user's question.\n",
        "  - Instructs the model to respond in natural, concise language.\n",
        "\n",
        "- **`chain`**  \n",
        "  A composed pipeline that:\n",
        "  - Runs two parallel steps:\n",
        "    - **`_search_query | retriever`**: Uses the `_search_query` logic to condense the question (if chat history exists), and then fetches relevant context via the `retriever` function.\n",
        "    - **`RunnablePassthrough()`**: Passes the original question directly.\n",
        "  - Fills the `prompt` with the retrieved context and question.\n",
        "  - Sends the filled prompt to the language model (`llm`) to generate a response.\n",
        "  - Parses and returns the final answer as a plain string.\n",
        "\n",
        "This setup ensures that the model's response is grounded in actual retrieved knowledge, improving relevance and accuracy.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bru0C7ox0aJY"
      },
      "outputs": [],
      "source": [
        "# Answer the question based only on the following context\n",
        "template = \"\"\"Answer the query:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "Use natural language and be concise.\n",
        "Answer:\"\"\"\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "chain = (\n",
        "    RunnableParallel(\n",
        "        {\n",
        "            \"context\": _search_query | retriever,\n",
        "            \"question\": RunnablePassthrough(),\n",
        "        }\n",
        "    )\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87XK7HSgWU-8"
      },
      "source": [
        "### `chain.invoke({\"question\": \"...\"})` — What It Does\n",
        "\n",
        "This command executes a full pipeline that answers a user’s question by retrieving relevant data from both structured and unstructured sources."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oBOP7MYrV3Af"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        },
        "id": "Xv4LdswWPvZe",
        "outputId": "a14823da-64d4-48e9-afa6-5ffaa611909b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Search query: What is the minimum score of Duolingo?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-21-1a77de7b1cb4>:12: LangChainDeprecationWarning: The function `remove_lucene_chars` was deprecated in LangChain 0.3.8 and will be removed in 1.0. An updated version of the function exists in the :meth:`~langchain-neo4j package and should be used instead. To use it run `pip install -U :meth:`~langchain-neo4j` and import as `from :meth:`~langchain_neo4j.vectorstores.neo4j_vector import remove_lucene_chars``.\n",
            "  words = [el for el in remove_lucene_chars(input).split() if el]\n",
            "WARNING:neo4j.notifications:Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.FeatureDeprecationWarning} {category: DEPRECATION} {title: This feature is deprecated and will be removed in future versions.} {description: CALL subquery without a variable scope clause is now deprecated. Use CALL (node, node) { ... }} {position: line: 3, column: 13, offset: 104} for query: \"CALL db.index.fulltext.queryNodes('entity', $query, {limit:2})\\n            YIELD node,score\\n            CALL {\\n              WITH node\\n              MATCH (node)-[r:!MENTIONS]->(neighbor)\\n              RETURN node.id + ' - ' + type(r) + ' -> ' + neighbor.id AS output\\n              UNION ALL\\n              WITH node\\n              MATCH (node)<-[r:!MENTIONS]-(neighbor)\\n              RETURN neighbor.id + ' - ' + type(r) + ' -> ' +  node.id AS output\\n            }\\n            RETURN output LIMIT 50\\n            \"\n",
            "WARNING:neo4j.notifications:Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.FeatureDeprecationWarning} {category: DEPRECATION} {title: This feature is deprecated and will be removed in future versions.} {description: CALL subquery without a variable scope clause is now deprecated. Use CALL (node, node) { ... }} {position: line: 3, column: 13, offset: 104} for query: \"CALL db.index.fulltext.queryNodes('entity', $query, {limit:2})\\n            YIELD node,score\\n            CALL {\\n              WITH node\\n              MATCH (node)-[r:!MENTIONS]->(neighbor)\\n              RETURN node.id + ' - ' + type(r) + ' -> ' + neighbor.id AS output\\n              UNION ALL\\n              WITH node\\n              MATCH (node)<-[r:!MENTIONS]-(neighbor)\\n              RETURN neighbor.id + ' - ' + type(r) + ' -> ' +  node.id AS output\\n            }\\n            RETURN output LIMIT 50\\n            \"\n",
            "WARNING:neo4j.notifications:Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.FeatureDeprecationWarning} {category: DEPRECATION} {title: This feature is deprecated and will be removed in future versions.} {description: CALL subquery without a variable scope clause is now deprecated. Use CALL () { ... }} {position: line: 1, column: 1, offset: 0} for query: \"CALL { CALL db.index.vector.queryNodes($index, $k, $embedding) YIELD node, score WITH collect({node:node, score:score}) AS nodes, max(score) AS max UNWIND nodes AS n RETURN n.node AS node, (n.score / max) AS score UNION CALL db.index.fulltext.queryNodes($keyword_index, $query, {limit: $k}) YIELD node, score WITH collect({node:node, score:score}) AS nodes, max(score) AS max UNWIND nodes AS n RETURN n.node AS node, (n.score / max) AS score } WITH node, max(score) AS score ORDER BY score DESC LIMIT $k RETURN reduce(str='', k IN ['text'] | str + '\\\\n' + k + ': ' + coalesce(node[k], '')) AS text, node {.*, `embedding`: Null, id: Null, `text`: Null} AS metadata, score\"\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The minimum score for Duolingo is 105.'"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chain.invoke({\"question\": \"What is the minimum score of Duolingo?\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "id": "L5C9R3Pp0lrw",
        "outputId": "747f66da-5f2e-4696-b587-2b517287dd6d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Search query: Is the minimum score for Dulingo 110 ?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:neo4j.notifications:Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.FeatureDeprecationWarning} {category: DEPRECATION} {title: This feature is deprecated and will be removed in future versions.} {description: CALL subquery without a variable scope clause is now deprecated. Use CALL (node, node) { ... }} {position: line: 3, column: 13, offset: 104} for query: \"CALL db.index.fulltext.queryNodes('entity', $query, {limit:2})\\n            YIELD node,score\\n            CALL {\\n              WITH node\\n              MATCH (node)-[r:!MENTIONS]->(neighbor)\\n              RETURN node.id + ' - ' + type(r) + ' -> ' + neighbor.id AS output\\n              UNION ALL\\n              WITH node\\n              MATCH (node)<-[r:!MENTIONS]-(neighbor)\\n              RETURN neighbor.id + ' - ' + type(r) + ' -> ' +  node.id AS output\\n            }\\n            RETURN output LIMIT 50\\n            \"\n",
            "WARNING:neo4j.notifications:Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.FeatureDeprecationWarning} {category: DEPRECATION} {title: This feature is deprecated and will be removed in future versions.} {description: CALL subquery without a variable scope clause is now deprecated. Use CALL (node, node) { ... }} {position: line: 3, column: 13, offset: 104} for query: \"CALL db.index.fulltext.queryNodes('entity', $query, {limit:2})\\n            YIELD node,score\\n            CALL {\\n              WITH node\\n              MATCH (node)-[r:!MENTIONS]->(neighbor)\\n              RETURN node.id + ' - ' + type(r) + ' -> ' + neighbor.id AS output\\n              UNION ALL\\n              WITH node\\n              MATCH (node)<-[r:!MENTIONS]-(neighbor)\\n              RETURN neighbor.id + ' - ' + type(r) + ' -> ' +  node.id AS output\\n            }\\n            RETURN output LIMIT 50\\n            \"\n",
            "WARNING:neo4j.notifications:Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.FeatureDeprecationWarning} {category: DEPRECATION} {title: This feature is deprecated and will be removed in future versions.} {description: CALL subquery without a variable scope clause is now deprecated. Use CALL () { ... }} {position: line: 1, column: 1, offset: 0} for query: \"CALL { CALL db.index.vector.queryNodes($index, $k, $embedding) YIELD node, score WITH collect({node:node, score:score}) AS nodes, max(score) AS max UNWIND nodes AS n RETURN n.node AS node, (n.score / max) AS score UNION CALL db.index.fulltext.queryNodes($keyword_index, $query, {limit: $k}) YIELD node, score WITH collect({node:node, score:score}) AS nodes, max(score) AS max UNWIND nodes AS n RETURN n.node AS node, (n.score / max) AS score } WITH node, max(score) AS score ORDER BY score DESC LIMIT $k RETURN reduce(str='', k IN ['text'] | str + '\\\\n' + k + ': ' + coalesce(node[k], '')) AS text, node {.*, `embedding`: Null, id: Null, `text`: Null} AS metadata, score\"\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'No, the minimum score for Duolingo is 105.'"
            ]
          },
          "execution_count": 55,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#True Positive\n",
        "chain.invoke({\"question\": \"Is the minimum score for Dulingo 110 ?\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "en1XQ9DlWU-_"
      },
      "source": [
        "### Disabling Warnings and Setting Logging Levels for Neo4j\n",
        "\n",
        "This code snippet is used to **suppress warnings** and **set logging levels** for specific parts of the code, especially in the context of working with the Neo4j database.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QAd4_yQDRQMv"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "import logging\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "logging.getLogger(\"neo4j.notifications\").setLevel(logging.ERROR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "duilHxHs1Ac9",
        "outputId": "62515589-c002-4154-d971-8bdfcf4cf866"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Search query: Is a  score of 100 not sufficient for Dulingo ?\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'A score of 100 on the Duolingo English Test is sufficient for meeting the English proficiency requirement at Northwest Missouri State University.'"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#True Negative\n",
        "chain.invoke({\"question\": \"Is a  score of 100 not sufficient for Dulingo ?\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "S7CARN8C1IRB",
        "outputId": "9bd91d24-9109-44bf-ac66-e415c6c183d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Search query: Is a  score of 115 not sufficient for Dulingo ?\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'A score of 115 is not sufficient for Duolingo.'"
            ]
          },
          "execution_count": 62,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# False Negative\n",
        "chain.invoke({\"question\": \"Is a  score of 115 not sufficient for Dulingo ?\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "FR1gMLPx1S0M",
        "outputId": "c8aa214f-6188-43f1-db92-5b426864957c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Search query: Is a  score of 105 sufficient for Dulingo ?\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Yes, a score of 105 is sufficient for Duolingo.'"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# False Positives\n",
        "chain.invoke({\"question\": \"Is a  score of 105 sufficient for Dulingo ?\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JHVxM45pR1p5"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "DJRGvroZ1cSJ",
        "outputId": "2a5bce34-ea76-4604-f4a8-aa3ccf66ab75"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Search query: Who is the Advisor for MSACS program?\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Dr. Ajay Bandi is the Advisor for the MSACS program at Northwest Missouri State University.'"
            ]
          },
          "execution_count": 63,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chain.invoke({\"question\": \"Who is the Advisor for MSACS program?\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "Q784P6LWt-ua",
        "outputId": "e0a084f2-4e5d-4e27-94b0-4a700a11fde3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Search query: What is his email ?\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"Ajay Bandi's email is ajay@nwmissouri.edu or msacs@nwmissouri.edu.\""
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chain.invoke({\"question\": \"What is his email ?\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "T1x2V51PuENT",
        "outputId": "80d490a8-e8e0-4fb9-ab50-c83c3e4a461f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Search query: What are the required courses ?\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The required courses for the MSACS program are:\\n- Database Systems\\n- Object-Oriented Programming\\n- Advanced Topics in Database Systems\\n- Developing Web Applications and Services\\n- Project Management in Business and Technology\\n- Mobile Computing: iOS or Mobile Computing: Android\\n- Application Design: Patterns And Frameworks\\n- CS Graduate Directed Project I\\n- CS Graduate Directed Project II\\n- Advisor Approved Elective-I\\n- Advisor Approved Elective-II'"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chain.invoke({\"question\": \"What are the required courses ?\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EjgwVXmyvqmM"
      },
      "outputs": [],
      "source": [
        "chain.invoke({\"question\": \"Is fire hot tell me T or F?\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EIkFhW5cWU_G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d51dec0-da35-4c7d-e00b-627e0b1864f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing main.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile main.py\n",
        "from langchain_core.runnables import (\n",
        "    RunnableBranch,\n",
        "    RunnableLambda,\n",
        "    RunnableParallel,\n",
        "    RunnablePassthrough,\n",
        ")\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.prompts.prompt import PromptTemplate\n",
        "from langchain_core.pydantic_v1 import BaseModel, Field\n",
        "from typing import Tuple, List, Optional\n",
        "from langchain_core.messages import AIMessage, HumanMessage\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "import os\n",
        "from langchain_community.graphs import Neo4jGraph\n",
        "from langchain.text_splitter import TokenTextSplitter\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_experimental.graph_transformers import LLMGraphTransformer\n",
        "from neo4j import GraphDatabase\n",
        "from langchain_community.vectorstores import Neo4jVector\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_community.vectorstores.neo4j_vector import remove_lucene_chars\n",
        "from langchain_core.runnables import ConfigurableField, RunnableParallel, RunnablePassthrough\n",
        "\n",
        "\n",
        "try:\n",
        "  import google.colab\n",
        "  from google.colab import output\n",
        "  output.enable_custom_widget_manager()\n",
        "except:\n",
        "  pass\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"\"#replace your OPEN_API key\n",
        "os.environ[\"NEO4J_URI\"] = \"\"#NEO4J URL\n",
        "os.environ[\"NEO4J_USERNAME\"] = \"neo4j\" #NEO4J USERNAME\n",
        "os.environ[\"NEO4J_PASSWORD\"] = \"\"#NEO4J PASSWORD\n",
        "\n",
        "_search_query = RunnableLambda(lambda x : x[\"question\"])\n",
        "\n",
        "\n",
        "graph = Neo4jGraph()\n",
        "\n",
        "llm=ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo-0125\") # gpt-4-0125-preview occasionally has issues\n",
        "llm_transformer = LLMGraphTransformer(llm=llm)\n",
        "\n",
        "vector_index = Neo4jVector.from_existing_graph(\n",
        "    OpenAIEmbeddings(),\n",
        "    search_type=\"hybrid\",\n",
        "    node_label=\"Document\",\n",
        "    text_node_properties=[\"text\"],\n",
        "    embedding_node_property=\"embedding\"\n",
        ")\n",
        "\n",
        "\n",
        "graph.query(\n",
        "    \"CREATE FULLTEXT INDEX entity IF NOT EXISTS FOR (e:__Entity__) ON EACH [e.id]\")\n",
        "\n",
        "# Extract entities from text\n",
        "class Entities(BaseModel):\n",
        "    \"\"\"Identifying information about entities.\"\"\"\n",
        "\n",
        "    names: List[str] = Field(\n",
        "        ...,\n",
        "        description=\"All the courses, requirements, or business entities that \"\n",
        "        \"appear in the text\",\n",
        "    )\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"You are extracting course, professors, requirements, project entities from the text.\",\n",
        "        ),\n",
        "        (\n",
        "            \"human\",\n",
        "            \"Use the given format to extract information from the following \"\n",
        "            \"input: {question}\",\n",
        "        ),\n",
        "    ]\n",
        ")\n",
        "\n",
        "entity_chain = prompt | llm.with_structured_output(Entities)\n",
        "\n",
        "def generate_full_text_query(input: str) -> str:\n",
        "    \"\"\"\n",
        "    Generate a full-text search query for a given input string.\n",
        "\n",
        "    This function constructs a query string suitable for a full-text search.\n",
        "    It processes the input string by splitting it into words and appending a\n",
        "    similarity threshold (~2 changed characters) to each word, then combines\n",
        "    them using the AND operator. Useful for mapping entities from user questions\n",
        "    to database values, and allows for some misspelings.\n",
        "    \"\"\"\n",
        "    full_text_query = \"\"\n",
        "    words = [el for el in remove_lucene_chars(input).split() if el]\n",
        "    for word in words[:-1]:\n",
        "        full_text_query += f\" {word}~2 AND\"\n",
        "    full_text_query += f\" {words[-1]}~2\"\n",
        "    return full_text_query.strip()\n",
        "\n",
        "# Fulltext index query\n",
        "def structured_retriever(question: str) -> str:\n",
        "    \"\"\"\n",
        "    Collects the neighborhood of entities mentioned\n",
        "    in the question\n",
        "    \"\"\"\n",
        "    result = \"\"\n",
        "    entities = entity_chain.invoke({\"question\": question})\n",
        "    for entity in entities.names:\n",
        "        response = graph.query(\n",
        "            \"\"\"CALL db.index.fulltext.queryNodes('entity', $query, {limit:2})\n",
        "            YIELD node,score\n",
        "            CALL {\n",
        "              WITH node\n",
        "              MATCH (node)-[r:!MENTIONS]->(neighbor)\n",
        "              RETURN node.id + ' - ' + type(r) + ' -> ' + neighbor.id AS output\n",
        "              UNION ALL\n",
        "              WITH node\n",
        "              MATCH (node)<-[r:!MENTIONS]-(neighbor)\n",
        "              RETURN neighbor.id + ' - ' + type(r) + ' -> ' +  node.id AS output\n",
        "            }\n",
        "            RETURN output LIMIT 50\n",
        "            \"\"\",\n",
        "            {\"query\": generate_full_text_query(entity)},\n",
        "        )\n",
        "        result += \"\\n\".join([el['output'] for el in response])\n",
        "    return result\n",
        "\n",
        "def retriever(question: str):\n",
        "    print(f\"Search query: {question}\")\n",
        "    structured_data = structured_retriever(question)\n",
        "    unstructured_data = [el.page_content for el in vector_index.similarity_search(question)]\n",
        "    final_data = f\"\"\" Structured data: {structured_data} Unstructured data: {\"#Document \". join(unstructured_data)}  \"\"\"\n",
        "    return final_data\n",
        "\n",
        "\n",
        "def answerquery(question: str):\n",
        "  template = \"\"\"Answer the question based only on the following context:\n",
        "  {context}\n",
        "\n",
        "  Question: {question}\n",
        "  Use natural language and be concise.\n",
        "  Answer:\"\"\"\n",
        "  prompt = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "  chain = (\n",
        "      RunnableParallel(\n",
        "          {\n",
        "              \"context\": _search_query | retriever,\n",
        "              \"question\": RunnablePassthrough(),\n",
        "          }\n",
        "      )\n",
        "      | prompt\n",
        "      | llm\n",
        "      | StrOutputParser()\n",
        "  )\n",
        "\n",
        "  return chain.invoke(({\"question\": question}))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UzFfyTKBZfoa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb4e59d8-23fa-4f10-80af-b9fdf1ae26d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "from main import answerquery\n",
        "\n",
        "st.title(\"NWMSU QA Chatbot\")\n",
        "\n",
        "# Initialize chat history\n",
        "if \"messages\" not in st.session_state:\n",
        "    st.session_state.messages = []\n",
        "\n",
        "# Display chat messages from history on app rerun\n",
        "for message in st.session_state.messages:\n",
        "    with st.chat_message(message[\"role\"]):\n",
        "        st.markdown(message[\"content\"])\n",
        "\n",
        "# React to user input\n",
        "if prompt := st.text_input(\"What is up?\"):\n",
        "    # Display user message in chat message container\n",
        "    st.chat_message(\"user\").markdown(prompt)\n",
        "    # Add user message to chat history\n",
        "    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
        "\n",
        "    response = answerquery(prompt)\n",
        "    # Display assistant response in chat message container\n",
        "    with st.chat_message(\"assistant\"):\n",
        "        st.markdown(response)\n",
        "    # Add assistant response to chat history\n",
        "    st.session_state.messages.append({\"role\": \"assistant\", \"content\": response})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NFaxmm4cZyu9",
        "outputId": "697ee574-03c3-4352-9d95-06b52b88a012"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
            "\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://34.150.230.255:8501\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Stopping...\u001b[0m\n",
            "\u001b[34m  Stopping...\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!streamlit run app.py"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "4fac87ddb3c24ea899f27c9dc57f42bf": {
          "model_module": "yfiles-jupyter-graphs",
          "model_module_version": "^1.10.1",
          "model_name": "GraphModel",
          "state": {
            "_context_pane_mapping": [
              {
                "id": "Neighborhood",
                "title": "Neighborhood"
              },
              {
                "id": "Data",
                "title": "Data"
              },
              {
                "id": "Search",
                "title": "Search"
              },
              {
                "id": "About",
                "title": "About"
              }
            ],
            "_data_importer": "neo4j",
            "_directed": true,
            "_dom_classes": [],
            "_edges": [
              {
                "color": "#607D8B",
                "directed": true,
                "end": 2,
                "id": 1152922604118474800,
                "label": "OFFERS",
                "properties": {
                  "label": "OFFERS"
                },
                "start": 1,
                "styles": {},
                "thickness_factor": 1
              },
              {
                "color": "#673AB7",
                "directed": true,
                "end": 3,
                "id": 1152923703630102500,
                "label": "COLLABORATE",
                "properties": {
                  "label": "COLLABORATE"
                },
                "start": 1,
                "styles": {},
                "thickness_factor": 1
              },
              {
                "color": "#CDDC39",
                "directed": true,
                "end": 16,
                "id": 1152931400211497000,
                "label": "LOCATION",
                "properties": {
                  "label": "LOCATION"
                },
                "start": 1,
                "styles": {},
                "thickness_factor": 1
              },
              {
                "color": "#CDDC39",
                "directed": true,
                "end": 257,
                "id": 6917538923245732000,
                "label": "LOCATION",
                "properties": {
                  "label": "LOCATION"
                },
                "start": 1,
                "styles": {},
                "thickness_factor": 1
              },
              {
                "color": "#9E9E9E",
                "directed": true,
                "end": 131,
                "id": 1152937997281263600,
                "label": "PROVIDES",
                "properties": {
                  "label": "PROVIDES"
                },
                "start": 1,
                "styles": {},
                "thickness_factor": 1
              },
              {
                "color": "#9E9E9E",
                "directed": true,
                "end": 132,
                "id": 1155189797094948900,
                "label": "PROVIDES",
                "properties": {
                  "label": "PROVIDES"
                },
                "start": 1,
                "styles": {},
                "thickness_factor": 1
              },
              {
                "color": "#9C27B0",
                "directed": true,
                "end": 134,
                "id": 1152983077258002400,
                "label": "OFFERED_AT",
                "properties": {
                  "label": "OFFERED_AT"
                },
                "start": 1,
                "styles": {},
                "thickness_factor": 1
              },
              {
                "color": "#2196F3",
                "directed": true,
                "end": 262,
                "id": 6917640078315487000,
                "label": "OFFER",
                "properties": {
                  "label": "OFFER"
                },
                "start": 1,
                "styles": {},
                "thickness_factor": 1
              },
              {
                "color": "#4CAF50",
                "directed": true,
                "end": 45,
                "id": 6917642277338743000,
                "label": "FOCUS",
                "properties": {
                  "label": "FOCUS"
                },
                "start": 1,
                "styles": {},
                "thickness_factor": 1
              },
              {
                "color": "#4CAF50",
                "directed": true,
                "end": 275,
                "id": 6917642277338743000,
                "label": "FOCUS",
                "properties": {
                  "label": "FOCUS"
                },
                "start": 1,
                "styles": {},
                "thickness_factor": 1
              },
              {
                "color": "#4CAF50",
                "directed": true,
                "end": 361,
                "id": 6917642277338743000,
                "label": "FOCUS",
                "properties": {
                  "label": "FOCUS"
                },
                "start": 1,
                "styles": {},
                "thickness_factor": 1
              },
              {
                "color": "#F44336",
                "directed": true,
                "end": 125,
                "id": 1153041351374274600,
                "label": "INTERACTION",
                "properties": {
                  "label": "INTERACTION"
                },
                "start": 1,
                "styles": {},
                "thickness_factor": 1
              },
              {
                "color": "#607D8B",
                "directed": true,
                "end": 258,
                "id": 6917649973920137000,
                "label": "COMPARISON",
                "properties": {
                  "label": "COMPARISON"
                },
                "start": 1,
                "styles": {},
                "thickness_factor": 1
              },
              {
                "color": "#673AB7",
                "directed": true,
                "end": 175,
                "id": 6917651073431765000,
                "label": "DESCRIPTION",
                "properties": {
                  "label": "DESCRIPTION"
                },
                "start": 1,
                "styles": {},
                "thickness_factor": 1
              },
              {
                "color": "#673AB7",
                "directed": true,
                "end": 191,
                "id": 6917651073431765000,
                "label": "DESCRIPTION",
                "properties": {
                  "label": "DESCRIPTION"
                },
                "start": 1,
                "styles": {},
                "thickness_factor": 1
              },
              {
                "color": "#673AB7",
                "directed": true,
                "end": 212,
                "id": 6917651073431765000,
                "label": "DESCRIPTION",
                "properties": {
                  "label": "DESCRIPTION"
                },
                "start": 1,
                "styles": {},
                "thickness_factor": 1
              },
              {
                "color": "#673AB7",
                "directed": true,
                "end": 259,
                "id": 6917651073431765000,
                "label": "DESCRIPTION",
                "properties": {
                  "label": "DESCRIPTION"
                },
                "start": 1,
                "styles": {},
                "thickness_factor": 1
              },
              {
                "color": "#673AB7",
                "directed": true,
                "end": 260,
                "id": 6917651073431765000,
                "label": "DESCRIPTION",
                "properties": {
                  "label": "DESCRIPTION"
                },
                "start": 1,
                "styles": {},
                "thickness_factor": 1
              },
              {
                "color": "#673AB7",
                "directed": true,
                "end": 261,
                "id": 6917651073431765000,
                "label": "DESCRIPTION",
                "properties": {
                  "label": "DESCRIPTION"
                },
                "start": 1,
                "styles": {},
                "thickness_factor": 1
              },
              {
                "color": "#673AB7",
                "directed": true,
                "end": 266,
                "id": 6917651073431765000,
                "label": "DESCRIPTION",
                "properties": {
                  "label": "DESCRIPTION"
                },
                "start": 1,
                "styles": {},
                "thickness_factor": 1
              },
              {
                "color": "#673AB7",
                "directed": true,
                "end": 273,
                "id": 6917651073431765000,
                "label": "DESCRIPTION",
                "properties": {
                  "label": "DESCRIPTION"
                },
                "start": 1,
                "styles": {},
                "thickness_factor": 1
              },
              {
                "color": "#673AB7",
                "directed": true,
                "end": 274,
                "id": 6917651073431765000,
                "label": "DESCRIPTION",
                "properties": {
                  "label": "DESCRIPTION"
                },
                "start": 1,
                "styles": {},
                "thickness_factor": 1
              },
              {
                "color": "#673AB7",
                "directed": true,
                "end": 275,
                "id": 6917651073431765000,
                "label": "DESCRIPTION",
                "properties": {
                  "label": "DESCRIPTION"
                },
                "start": 1,
                "styles": {},
                "thickness_factor": 1
              },
              {
                "color": "#673AB7",
                "directed": true,
                "end": 287,
                "id": 6917651073431765000,
                "label": "DESCRIPTION",
                "properties": {
                  "label": "DESCRIPTION"
                },
                "start": 1,
                "styles": {},
                "thickness_factor": 1
              },
              {
                "color": "#CDDC39",
                "directed": true,
                "end": 262,
                "id": 6917652172943393000,
                "label": "AVAILABILITY",
                "properties": {
                  "label": "AVAILABILITY"
                },
                "start": 1,
                "styles": {},
                "thickness_factor": 1
              },
              {
                "color": "#9E9E9E",
                "directed": true,
                "end": 263,
                "id": 6917653272455021000,
                "label": "OFFERING",
                "properties": {
                  "label": "OFFERING"
                },
                "start": 1,
                "styles": {},
                "thickness_factor": 1
              },
              {
                "color": "#9E9E9E",
                "directed": true,
                "end": 264,
                "id": 6917653272455021000,
                "label": "OFFERING",
                "properties": {
                  "label": "OFFERING"
                },
                "start": 1,
                "styles": {},
                "thickness_factor": 1
              },
              {
                "color": "#9E9E9E",
                "directed": true,
                "end": 265,
                "id": 6917653272455021000,
                "label": "OFFERING",
                "properties": {
                  "label": "OFFERING"
                },
                "start": 1,
                "styles": {},
                "thickness_factor": 1
              },
              {
                "color": "#9C27B0",
                "directed": true,
                "end": 267,
                "id": 6917654371966648000,
                "label": "OUTCOME",
                "properties": {
                  "label": "OUTCOME"
                },
                "start": 1,
                "styles": {},
                "thickness_factor": 1
              },
              {
                "color": "#2196F3",
                "directed": true,
                "end": 268,
                "id": 6917655471478276000,
                "label": "FUNDING",
                "properties": {
                  "label": "FUNDING"
                },
                "start": 1,
                "styles": {},
                "thickness_factor": 1
              },
              {
                "color": "#4CAF50",
                "directed": true,
                "end": 269,
                "id": 6917656570989904000,
                "label": "FUNDING_SOURCE",
                "properties": {
                  "label": "FUNDING_SOURCE"
                },
                "start": 1,
                "styles": {},
                "thickness_factor": 1
              },
              {
                "color": "#F44336",
                "directed": true,
                "end": 270,
                "id": 6917657670501532000,
                "label": "SUPPORT",
                "properties": {
                  "label": "SUPPORT"
                },
                "start": 1,
                "styles": {},
                "thickness_factor": 1
              },
              {
                "color": "#F44336",
                "directed": true,
                "end": 271,
                "id": 6917657670501532000,
                "label": "SUPPORT",
                "properties": {
                  "label": "SUPPORT"
                },
                "start": 1,
                "styles": {},
                "thickness_factor": 1
              },
              {
                "color": "#F44336",
                "directed": true,
                "end": 272,
                "id": 6917657670501532000,
                "label": "SUPPORT",
                "properties": {
                  "label": "SUPPORT"
                },
                "start": 1,
                "styles": {},
                "thickness_factor": 1
              },
              {
                "color": "#607D8B",
                "directed": true,
                "end": 276,
                "id": 6917658770013159000,
                "label": "ENCOURAGEMENT",
                "properties": {
                  "label": "ENCOURAGEMENT"
                },
                "start": 1,
                "styles": {},
                "thickness_factor": 1
              },
              {
                "color": "#673AB7",
                "directed": true,
                "end": 277,
                "id": 6917659869524787000,
                "label": "REFERENCE",
                "properties": {
                  "label": "REFERENCE"
                },
                "start": 1,
                "styles": {},
                "thickness_factor": 1
              },
              {
                "color": "#CDDC39",
                "directed": true,
                "end": 280,
                "id": 6917662068548043000,
                "label": "INCLUSION",
                "properties": {
                  "label": "INCLUSION"
                },
                "start": 1,
                "styles": {},
                "thickness_factor": 1
              },
              {
                "color": "#9E9E9E",
                "directed": true,
                "end": 288,
                "id": 6917665367082926000,
                "label": "CLARIFICATION",
                "properties": {
                  "label": "CLARIFICATION"
                },
                "start": 1,
                "styles": {},
                "thickness_factor": 1
              },
              {
                "color": "#9C27B0",
                "directed": true,
                "end": 125,
                "id": 1153144705467285500,
                "label": "ATTRIBUTE",
                "properties": {
                  "label": "ATTRIBUTE"
                },
                "start": 1,
                "styles": {},
                "thickness_factor": 1
              },
              {
                "color": "#9C27B0",
                "directed": true,
                "end": 191,
                "id": 6917752228501520000,
                "label": "ATTRIBUTE",
                "properties": {
                  "label": "ATTRIBUTE"
                },
                "start": 1,
                "styles": {},
                "thickness_factor": 1
              },
              {
                "color": "#9C27B0",
                "directed": true,
                "end": 258,
                "id": 1155396505280970800,
                "label": "ATTRIBUTE",
                "properties": {
                  "label": "ATTRIBUTE"
                },
                "start": 1,
                "styles": {},
                "thickness_factor": 1
              },
              {
                "color": "#9C27B0",
                "directed": true,
                "end": 259,
                "id": 1157648305094656000,
                "label": "ATTRIBUTE",
                "properties": {
                  "label": "ATTRIBUTE"
                },
                "start": 1,
                "styles": {},
                "thickness_factor": 1
              },
              {
                "color": "#9C27B0",
                "directed": true,
                "end": 260,
                "id": 1159900104908341200,
                "label": "ATTRIBUTE",
                "properties": {
                  "label": "ATTRIBUTE"
                },
                "start": 1,
                "styles": {},
                "thickness_factor": 1
              },
              {
                "color": "#9C27B0",
                "directed": true,
                "end": 261,
                "id": 1162151904722026500,
                "label": "ATTRIBUTE",
                "properties": {
                  "label": "ATTRIBUTE"
                },
                "start": 1,
                "styles": {},
                "thickness_factor": 1
              },
              {
                "color": "#9C27B0",
                "directed": true,
                "end": 262,
                "id": 1164403704535711700,
                "label": "ATTRIBUTE",
                "properties": {
                  "label": "ATTRIBUTE"
                },
                "start": 1,
                "styles": {},
                "thickness_factor": 1
              },
              {
                "color": "#9C27B0",
                "directed": true,
                "end": 266,
                "id": 1168907304163082200,
                "label": "ATTRIBUTE",
                "properties": {
                  "label": "ATTRIBUTE"
                },
                "start": 1,
                "styles": {},
                "thickness_factor": 1
              },
              {
                "color": "#9C27B0",
                "directed": true,
                "end": 275,
                "id": 1166655504349397000,
                "label": "ATTRIBUTE",
                "properties": {
                  "label": "ATTRIBUTE"
                },
                "start": 1,
                "styles": {},
                "thickness_factor": 1
              },
              {
                "color": "#9C27B0",
                "directed": true,
                "end": 280,
                "id": 1171159103976767500,
                "label": "ATTRIBUTE",
                "properties": {
                  "label": "ATTRIBUTE"
                },
                "start": 1,
                "styles": {},
                "thickness_factor": 1
              },
              {
                "color": "#9C27B0",
                "directed": true,
                "end": 361,
                "id": 1173410903790452700,
                "label": "ATTRIBUTE",
                "properties": {
                  "label": "ATTRIBUTE"
                },
                "start": 1,
                "styles": {},
                "thickness_factor": 1
              },
              {
                "color": "#2196F3",
                "directed": true,
                "end": 191,
                "id": 6917803905548026000,
                "label": "FEATURE",
                "properties": {
                  "label": "FEATURE"
                },
                "start": 1,
                "styles": {},
                "thickness_factor": 1
              }
            ],
            "_graph_layout": {},
            "_highlight": [],
            "_license": {},
            "_model_module": "yfiles-jupyter-graphs",
            "_model_module_version": "^1.10.1",
            "_model_name": "GraphModel",
            "_neighborhood": {},
            "_nodes": [
              {
                "color": "#2196F3",
                "id": 1,
                "label": "Northwest",
                "position": [
                  0,
                  0
                ],
                "properties": {
                  "id": "Northwest",
                  "label": "__Entity__:University:Location:Organization:Campus:Institution"
                },
                "scale_factor": 1,
                "size": [
                  55,
                  55
                ],
                "styles": {},
                "type": "#2196F3"
              },
              {
                "color": "#4CAF50",
                "id": 2,
                "label": "Ms-Acs",
                "position": [
                  0,
                  0
                ],
                "properties": {
                  "id": "Ms-Acs",
                  "label": "__Entity__:Program"
                },
                "scale_factor": 1,
                "size": [
                  55,
                  55
                ],
                "styles": {},
                "type": "#4CAF50"
              },
              {
                "color": "#F44336",
                "id": 3,
                "label": "Faculty Members",
                "position": [
                  0,
                  0
                ],
                "properties": {
                  "id": "Faculty Members",
                  "label": "__Entity__:Academic staff:Person"
                },
                "scale_factor": 1,
                "size": [
                  55,
                  55
                ],
                "styles": {},
                "type": "#F44336"
              },
              {
                "color": "#607D8B",
                "id": 16,
                "label": "Northwest Missouri State University",
                "position": [
                  0,
                  0
                ],
                "properties": {
                  "id": "Northwest Missouri State University",
                  "label": "__Entity__:University:Institution:Organization"
                },
                "scale_factor": 1,
                "size": [
                  55,
                  55
                ],
                "styles": {},
                "type": "#607D8B"
              },
              {
                "color": "#673AB7",
                "id": 257,
                "label": "United States",
                "position": [
                  0,
                  0
                ],
                "properties": {
                  "id": "United States",
                  "label": "__Entity__:Country"
                },
                "scale_factor": 1,
                "size": [
                  55,
                  55
                ],
                "styles": {},
                "type": "#673AB7"
              },
              {
                "color": "#CDDC39",
                "id": 131,
                "label": "International Students",
                "position": [
                  0,
                  0
                ],
                "properties": {
                  "id": "International Students",
                  "label": "__Entity__:Student:Person"
                },
                "scale_factor": 1,
                "size": [
                  55,
                  55
                ],
                "styles": {},
                "type": "#CDDC39"
              },
              {
                "color": "#9E9E9E",
                "id": 132,
                "label": "Scholarship",
                "position": [
                  0,
                  0
                ],
                "properties": {
                  "id": "Scholarship",
                  "label": "__Entity__:Financial aid:Financial support"
                },
                "scale_factor": 1,
                "size": [
                  55,
                  55
                ],
                "styles": {},
                "type": "#9E9E9E"
              },
              {
                "color": "#4CAF50",
                "id": 134,
                "label": "On-Campus Employment",
                "position": [
                  0,
                  0
                ],
                "properties": {
                  "id": "On-Campus Employment",
                  "label": "__Entity__:Program"
                },
                "scale_factor": 1,
                "size": [
                  55,
                  55
                ],
                "styles": {},
                "type": "#4CAF50"
              },
              {
                "color": "#9C27B0",
                "id": 262,
                "label": "Scholarships",
                "position": [
                  0,
                  0
                ],
                "properties": {
                  "id": "Scholarships",
                  "label": "__Entity__:Financial assistance:Scholarships"
                },
                "scale_factor": 1,
                "size": [
                  55,
                  55
                ],
                "styles": {},
                "type": "#9C27B0"
              },
              {
                "color": "#2196F3",
                "id": 45,
                "label": "Computer Science",
                "position": [
                  0,
                  0
                ],
                "properties": {
                  "id": "Computer Science",
                  "label": "__Entity__:Subject:Field:Field of study"
                },
                "scale_factor": 1,
                "size": [
                  55,
                  55
                ],
                "styles": {},
                "type": "#2196F3"
              },
              {
                "color": "#4CAF50",
                "id": 275,
                "label": "Programs",
                "position": [
                  0,
                  0
                ],
                "properties": {
                  "id": "Programs",
                  "label": "__Entity__:Programs:Academic program:Program"
                },
                "scale_factor": 1,
                "size": [
                  55,
                  55
                ],
                "styles": {},
                "type": "#4CAF50"
              },
              {
                "color": "#F44336",
                "id": 361,
                "label": "Research",
                "position": [
                  0,
                  0
                ],
                "properties": {
                  "id": "Research",
                  "label": "__Entity__:Research:Activity"
                },
                "scale_factor": 1,
                "size": [
                  55,
                  55
                ],
                "styles": {},
                "type": "#F44336"
              },
              {
                "color": "#607D8B",
                "id": 125,
                "label": "Faculty",
                "position": [
                  0,
                  0
                ],
                "properties": {
                  "id": "Faculty",
                  "label": "__Entity__:Academic staff:Person:Faculty"
                },
                "scale_factor": 1,
                "size": [
                  55,
                  55
                ],
                "styles": {},
                "type": "#607D8B"
              },
              {
                "color": "#673AB7",
                "id": 258,
                "label": "Tuition",
                "position": [
                  0,
                  0
                ],
                "properties": {
                  "id": "Tuition",
                  "label": "__Entity__:Tuition:Financial"
                },
                "scale_factor": 1,
                "size": [
                  55,
                  55
                ],
                "styles": {},
                "type": "#673AB7"
              },
              {
                "color": "#CDDC39",
                "id": 175,
                "label": "Graduates",
                "position": [
                  0,
                  0
                ],
                "properties": {
                  "id": "Graduates",
                  "label": "Concept:Person:__Entity__:Graduates:Academic"
                },
                "scale_factor": 1,
                "size": [
                  55,
                  55
                ],
                "styles": {},
                "type": "#CDDC39"
              },
              {
                "color": "#9E9E9E",
                "id": 191,
                "label": "Campus",
                "position": [
                  0,
                  0
                ],
                "properties": {
                  "id": "Campus",
                  "label": "__Entity__:Concept:Campus"
                },
                "scale_factor": 1,
                "size": [
                  55,
                  55
                ],
                "styles": {},
                "type": "#9E9E9E"
              },
              {
                "color": "#9C27B0",
                "id": 212,
                "label": "Undergraduates",
                "position": [
                  0,
                  0
                ],
                "properties": {
                  "id": "Undergraduates",
                  "label": "__Entity__:Student:Students:Undergraduates"
                },
                "scale_factor": 1,
                "size": [
                  55,
                  55
                ],
                "styles": {},
                "type": "#9C27B0"
              },
              {
                "color": "#2196F3",
                "id": 259,
                "label": "Trees",
                "position": [
                  0,
                  0
                ],
                "properties": {
                  "id": "Trees",
                  "label": "__Entity__:Trees:Natural feature"
                },
                "scale_factor": 1,
                "size": [
                  55,
                  55
                ],
                "styles": {},
                "type": "#2196F3"
              },
              {
                "color": "#4CAF50",
                "id": 260,
                "label": "Species",
                "position": [
                  0,
                  0
                ],
                "properties": {
                  "id": "Species",
                  "label": "__Entity__:Species:Natural feature"
                },
                "scale_factor": 1,
                "size": [
                  55,
                  55
                ],
                "styles": {},
                "type": "#4CAF50"
              },
              {
                "color": "#F44336",
                "id": 261,
                "label": "Arboretum",
                "position": [
                  0,
                  0
                ],
                "properties": {
                  "id": "Arboretum",
                  "label": "__Entity__:Arboretum:Natural feature"
                },
                "scale_factor": 1,
                "size": [
                  55,
                  55
                ],
                "styles": {},
                "type": "#F44336"
              },
              {
                "color": "#607D8B",
                "id": 266,
                "label": "Enrollment",
                "position": [
                  0,
                  0
                ],
                "properties": {
                  "id": "Enrollment",
                  "label": "__Entity__:Enrollment:Academic"
                },
                "scale_factor": 1,
                "size": [
                  55,
                  55
                ],
                "styles": {},
                "type": "#607D8B"
              },
              {
                "color": "#673AB7",
                "id": 273,
                "label": "Recognition",
                "position": [
                  0,
                  0
                ],
                "properties": {
                  "id": "Recognition",
                  "label": "__Entity__:Recognition:Acknowledgment"
                },
                "scale_factor": 1,
                "size": [
                  55,
                  55
                ],
                "styles": {},
                "type": "#673AB7"
              },
              {
                "color": "#CDDC39",
                "id": 274,
                "label": "Quality",
                "position": [
                  0,
                  0
                ],
                "properties": {
                  "id": "Quality",
                  "label": "__Entity__:Attribute:Quality"
                },
                "scale_factor": 1,
                "size": [
                  55,
                  55
                ],
                "styles": {},
                "type": "#CDDC39"
              },
              {
                "color": "#9E9E9E",
                "id": 287,
                "label": "Research Activities",
                "position": [
                  0,
                  0
                ],
                "properties": {
                  "id": "Research Activities",
                  "label": "__Entity__:Research activity"
                },
                "scale_factor": 1,
                "size": [
                  55,
                  55
                ],
                "styles": {},
                "type": "#9E9E9E"
              },
              {
                "color": "#9C27B0",
                "id": 263,
                "label": "Bachelor’S Degree Programs",
                "position": [
                  0,
                  0
                ],
                "properties": {
                  "id": "Bachelor’S Degree Programs",
                  "label": "__Entity__:Degree program"
                },
                "scale_factor": 1,
                "size": [
                  55,
                  55
                ],
                "styles": {},
                "type": "#9C27B0"
              },
              {
                "color": "#9C27B0",
                "id": 264,
                "label": "Master’S Degree Programs",
                "position": [
                  0,
                  0
                ],
                "properties": {
                  "id": "Master’S Degree Programs",
                  "label": "__Entity__:Degree program"
                },
                "scale_factor": 1,
                "size": [
                  55,
                  55
                ],
                "styles": {},
                "type": "#9C27B0"
              },
              {
                "color": "#2196F3",
                "id": 265,
                "label": "Ms-Acs Program",
                "position": [
                  0,
                  0
                ],
                "properties": {
                  "id": "Ms-Acs Program",
                  "label": "__Entity__:Program:Degree program"
                },
                "scale_factor": 1,
                "size": [
                  55,
                  55
                ],
                "styles": {},
                "type": "#2196F3"
              },
              {
                "color": "#4CAF50",
                "id": 267,
                "label": "Jobs",
                "position": [
                  0,
                  0
                ],
                "properties": {
                  "id": "Jobs",
                  "label": "__Entity__:Jobs:Employment"
                },
                "scale_factor": 1,
                "size": [
                  55,
                  55
                ],
                "styles": {},
                "type": "#4CAF50"
              },
              {
                "color": "#F44336",
                "id": 268,
                "label": "Grant",
                "position": [
                  0,
                  0
                ],
                "properties": {
                  "id": "Grant",
                  "label": "__Entity__:Grant:Financial"
                },
                "scale_factor": 1,
                "size": [
                  55,
                  55
                ],
                "styles": {},
                "type": "#F44336"
              },
              {
                "color": "#607D8B",
                "id": 269,
                "label": "National Science Foundation",
                "position": [
                  0,
                  0
                ],
                "properties": {
                  "id": "National Science Foundation",
                  "label": "__Entity__:Organization"
                },
                "scale_factor": 1,
                "size": [
                  55,
                  55
                ],
                "styles": {},
                "type": "#607D8B"
              },
              {
                "color": "#673AB7",
                "id": 270,
                "label": "Recruitment",
                "position": [
                  0,
                  0
                ],
                "properties": {
                  "id": "Recruitment",
                  "label": "__Entity__:Recruitment:Activity"
                },
                "scale_factor": 1,
                "size": [
                  55,
                  55
                ],
                "styles": {},
                "type": "#673AB7"
              },
              {
                "color": "#CDDC39",
                "id": 271,
                "label": "Retention",
                "position": [
                  0,
                  0
                ],
                "properties": {
                  "id": "Retention",
                  "label": "__Entity__:Retention:Activity"
                },
                "scale_factor": 1,
                "size": [
                  55,
                  55
                ],
                "styles": {},
                "type": "#CDDC39"
              },
              {
                "color": "#9E9E9E",
                "id": 272,
                "label": "Mentoring Program",
                "position": [
                  0,
                  0
                ],
                "properties": {
                  "id": "Mentoring Program",
                  "label": "__Entity__:Program:Mentoring program"
                },
                "scale_factor": 1,
                "size": [
                  55,
                  55
                ],
                "styles": {},
                "type": "#9E9E9E"
              },
              {
                "color": "#9C27B0",
                "id": 276,
                "label": "Questions",
                "position": [
                  0,
                  0
                ],
                "properties": {
                  "id": "Questions",
                  "label": "__Entity__:Inquiry:Questions"
                },
                "scale_factor": 1,
                "size": [
                  55,
                  55
                ],
                "styles": {},
                "type": "#9C27B0"
              },
              {
                "color": "#2196F3",
                "id": 277,
                "label": "Program Website",
                "position": [
                  0,
                  0
                ],
                "properties": {
                  "id": "Program Website",
                  "label": "__Entity__:Website"
                },
                "scale_factor": 1,
                "size": [
                  55,
                  55
                ],
                "styles": {},
                "type": "#2196F3"
              },
              {
                "color": "#4CAF50",
                "id": 280,
                "label": "Disciplines",
                "position": [
                  0,
                  0
                ],
                "properties": {
                  "id": "Disciplines",
                  "label": "__Entity__:Disciplines:Field of study"
                },
                "scale_factor": 1,
                "size": [
                  55,
                  55
                ],
                "styles": {},
                "type": "#4CAF50"
              },
              {
                "color": "#F44336",
                "id": 288,
                "label": "Research Institution",
                "position": [
                  0,
                  0
                ],
                "properties": {
                  "id": "Research Institution",
                  "label": "__Entity__:Institution"
                },
                "scale_factor": 1,
                "size": [
                  55,
                  55
                ],
                "styles": {},
                "type": "#F44336"
              }
            ],
            "_overview": {
              "enabled": null,
              "overview_set": false
            },
            "_selected_graph": [
              [],
              []
            ],
            "_sidebar": {
              "enabled": false,
              "start_with": null
            },
            "_view_count": null,
            "_view_module": "yfiles-jupyter-graphs",
            "_view_module_version": "^1.10.1",
            "_view_name": "GraphView",
            "layout": "IPY_MODEL_83960149f86840c4b4a1a55052e1cecd",
            "tabbable": null,
            "tooltip": null
          }
        },
        "83960149f86840c4b4a1a55052e1cecd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "2.0.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": "800px",
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "100%"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}